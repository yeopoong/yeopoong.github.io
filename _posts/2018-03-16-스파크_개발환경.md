---
layout: post
title: "스파크 개발환경"
categories: Devlog
tags: spark
---

## 이클립스 개발환경 설정 

### Scala IDE plug-in 설치

1. Eclipse Marketplace 에서 scala 로 검색
2. Scala IDE 4.2.x 를 Install

### Eclipse Scala Project plug-in 설치

* m2eclipse-scala 설치
1. Eclipse > Help > Install New software
2. Work with: http://alchim31.free.fr/m2e-scala/update-site 입력하고 엔터
3. Maven Integration for Eclipse 를 선택하고 설치한다. 

### Using the IDE with a 2.10 project 

1. Eclipse Project 의 Properties > Scala Compiler 설정 화면
2. Scala Compiler > Standard > Additional command line parameters 에 아래와 같이 입력
```
 -Xsource:2.10 -Ymacro-expand:none
```

Note: Scala Library 와 Spark Library 의 버전 호환성 해결(?) -> Scala IDE 의 문제로 보임

### Maven 설정
> Spark Application 개발을 위해서 설정된 메이븐 플러그인과 라이브러리는 다음과 같다.

#### libraries
1. scala-library 
1. spark-core
1. slf4j & logback
1. junit

#### plug-in
1. scala-maven-plugin   
java & scala mix project를 위해서 사용
2. maven-shade-plugin  
spark application packaging 을 위해서 사용

```
<dependency>
    <groupId>org.apache.spark</groupId>
    <artifactId>spark-core_2.10</artifactId>
    <version>2.0.0</version>
</dependency>
```

## Spark Shell

### 단일 머신 환경에서 실행하는 경우
```
$ spark-shell --master local
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.2.1
      /_/

Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_162)
Type in expressions to have them evaluated.
Type :help for more information.
```

### 클러스터 환경에서 실행하는 경우
```
$ spark-shell --master yarn-client --executor-cores 1 --num-executors 3
```

## Spark Submit

### spark-submit 플래그

| 플래그 | 설명
|:-------|:---
|--master|접속할 클러스터 매니저: yarn, local, spark, mesos
|--deploy-mode|드라이버 프로그램이 지역적으로 실행될지 클러스터의 작업 머신들 중에서 실행될지 결정
|--class|자바나 스칼라 프로그램을 실행할 때 'main'이 들어 있는 클래스
|--queue|스파크 어플리케이션을 수행하기 위해서 할당된 큐
|--num-executors|익스큐터 개수(디폴트:2)
|--executor-memory|익스큐터가 쓸 메모리를 바이트 단위로 지정
|--executor-cores|익스큐터가 사용할 CPU Core 개수(디폴트:1)

```
$spark-submit \
    --master yarn \
    --deploy-mode client \
    --queue root.wfbm \
    --class com.example.WordCount \
    --num-executors 10 \
    --executor-memory 16G \
    --executor-cores 5 \
    target/spark.jar \

$spark-submit --help
```

Spark Kill Running Application

```
$ ps aux | grep spark | grep WordCount
$ kill -9 pid

or

$ yarn application -kill application_1489644966516_0388
```

## Spark shell application UI

```
http://spark:4040
```
## Apache Spark Plugin

```xml
<dependency>
  <groupId>org.apache.phoenix</groupId>
  <artifactId>phoenix-spark</artifactId>
  <version>4.9.0-HBase-1.2</version>
</dependency>
```

## 로컬 테스트 시 winutils.exe 오류 
> 환경변수에 HADOOP_HOME 을 설정하여 해결

1. first download winutils.exe  
: http://public-repo-1.hortonworks.com/hdp-win-alpha/winutils.exe 
2. create folder like c:\winutils\bin
3. copy winutils.exe insight c:\winutils\bin
4. set path HADOOP_HOME c:\winutils

## JDK Install

```
$ wget --no-check-certificate --no-cookies --header "Cookie: oraclelicense=accept-securebackup-cookie" http://download.oracle.com/otn-pub/java/jdk/8u144-b01/090f390dda5b47b9b721c7dfaa008135/jdk-8u144-linux-x64.rpm
```

## 참조
* http://scala-ide.org/blog/Xsource-compatibility.html
* http://spark.apache.org/downloads.html
* http://phoenix.apache.org/phoenix_spark.html
